# -*- coding: utf-8 -*-
"""
===================================
Aè‚¡è‡ªé€‰è‚¡æ™ºèƒ½åˆ†æç³»ç»Ÿ - ä¸»è°ƒåº¦ç¨‹åº
===================================

èŒè´£ï¼š
1. åè°ƒå„æ¨¡å—å®Œæˆè‚¡ç¥¨åˆ†ææµç¨‹
2. å®ç°ä½å¹¶å‘çš„çº¿ç¨‹æ± è°ƒåº¦
3. å…¨å±€å¼‚å¸¸å¤„ç†ï¼Œç¡®ä¿å•è‚¡å¤±è´¥ä¸å½±å“æ•´ä½“
4. æä¾›å‘½ä»¤è¡Œå…¥å£

ä½¿ç”¨æ–¹å¼ï¼š
    python main.py              # æ­£å¸¸è¿è¡Œ
    python main.py --debug      # è°ƒè¯•æ¨¡å¼
    python main.py --dry-run    # ä»…è·å–æ•°æ®ä¸åˆ†æ

äº¤æ˜“ç†å¿µï¼ˆå·²èå…¥åˆ†æï¼‰ï¼š
- ä¸¥è¿›ç­–ç•¥ï¼šä¸è¿½é«˜ï¼Œä¹–ç¦»ç‡ > 5% ä¸ä¹°å…¥
- è¶‹åŠ¿äº¤æ˜“ï¼šåªåš MA5>MA10>MA20 å¤šå¤´æ’åˆ—
- æ•ˆç‡ä¼˜å…ˆï¼šå…³æ³¨ç­¹ç é›†ä¸­åº¦å¥½çš„è‚¡ç¥¨
- ä¹°ç‚¹åå¥½ï¼šç¼©é‡å›è¸© MA5/MA10 æ”¯æ’‘
"""
import os

# ä»£ç†é…ç½® - ä»…åœ¨æœ¬åœ°ç¯å¢ƒä½¿ç”¨ï¼ŒGitHub Actions ä¸éœ€è¦
if os.getenv("GITHUB_ACTIONS") != "true":
    pass

import argparse
import logging
import sys
import time
import json  # <--- å·²æ·»åŠ 
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, date, timezone, timedelta
from logging.handlers import RotatingFileHandler
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
from feishu_doc import FeishuDocManager

from config import get_config, Config
from storage import get_db, DatabaseManager
from data_provider import DataFetcherManager
from data_provider.akshare_fetcher import AkshareFetcher, RealtimeQuote, ChipDistribution
from analyzer import GeminiAnalyzer, AnalysisResult, STOCK_NAME_MAP
from notification import NotificationService, NotificationChannel, send_daily_report
from search_service import SearchService, SearchResponse
from stock_analyzer import StockTrendAnalyzer, TrendAnalysisResult
from market_analyzer import MarketAnalyzer

# é…ç½®æ—¥å¿—æ ¼å¼
LOG_FORMAT = '%(asctime)s | %(levelname)-8s | %(name)-20s | %(message)s'
LOG_DATE_FORMAT = '%Y-%m-%d %H:%M:%S'


def setup_logging(debug: bool = False, log_dir: str = "./logs") -> None:
    """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
    level = logging.DEBUG if debug else logging.INFO
    log_path = Path(log_dir)
    log_path.mkdir(parents=True, exist_ok=True)
    today_str = datetime.now().strftime('%Y%m%d')
    log_file = log_path / f"stock_analysis_{today_str}.log"
    debug_log_file = log_path / f"stock_analysis_debug_{today_str}.log"
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.DEBUG)
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(level)
    console_handler.setFormatter(logging.Formatter(LOG_FORMAT, LOG_DATE_FORMAT))
    root_logger.addHandler(console_handler)
    file_handler = RotatingFileHandler(log_file, maxBytes=10 * 1024 * 1024, backupCount=5, encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    file_handler.setFormatter(logging.Formatter(LOG_FORMAT, LOG_DATE_FORMAT))
    root_logger.addHandler(file_handler)
    debug_handler = RotatingFileHandler(debug_log_file, maxBytes=50 * 1024 * 1024, backupCount=3, encoding='utf-8')
    debug_handler.setLevel(logging.DEBUG)
    debug_handler.setFormatter(logging.Formatter(LOG_FORMAT, LOG_DATE_FORMAT))
    root_logger.addHandler(debug_handler)
    logging.getLogger('urllib3').setLevel(logging.WARNING)
    logging.getLogger('sqlalchemy').setLevel(logging.WARNING)
    logging.getLogger('google').setLevel(logging.WARNING)
    logging.getLogger('httpx').setLevel(logging.WARNING)
    logging.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œæ—¥å¿—ç›®å½•: {log_path.absolute()}")


logger = logging.getLogger(__name__)

# ==========================================
# æ–°å¢é…ç½®ï¼šè‚¡ç¥¨è¡Œä¸šæ˜ å°„è¡¨
# Key: è‚¡ç¥¨ä»£ç  (å»é™¤åç¼€), Value: è¡Œä¸šæ ‡ç­¾
# ==========================================
STOCK_SECTOR_MAP = {
    "603098": "Industrial", "NVDA": "Tech", "AAPL": "Tech",
    "TSLA": "Energy", "00700": "Tech", "600519": "Consumer",
    "BTC": "Crypto", "SPY": "Macro", "QQQ": "Macro", "300300": "Macro"
}
DEFAULT_SECTOR = "Macro"

class StockAnalysisPipeline:
    """è‚¡ç¥¨åˆ†æä¸»æµç¨‹è°ƒåº¦å™¨"""
    
    def __init__(self, config: Optional[Config] = None, max_workers: Optional[int] = None):
        """åˆå§‹åŒ–è°ƒåº¦å™¨"""
        self.config = config or get_config()
        self.max_workers = max_workers or self.config.max_workers
        
        # åˆå§‹åŒ–å„æ¨¡å— (è¿™äº›å¿…é¡»åœ¨ __init__ å†…éƒ¨å®Œæˆ)
        self.db = get_db()
        self.fetcher_manager = DataFetcherManager()
        self.akshare_fetcher = AkshareFetcher()
        self.trend_analyzer = StockTrendAnalyzer()
        self.analyzer = GeminiAnalyzer()
        self.notifier = NotificationService()
        
        # åˆå§‹åŒ–æœç´¢æœåŠ¡
        self.search_service = SearchService(
            bocha_keys=self.config.bocha_api_keys,
            tavily_keys=self.config.tavily_api_keys,
            serpapi_keys=self.config.serpapi_keys,
        )
        
        logger.info(f"è°ƒåº¦å™¨åˆå§‹åŒ–å®Œæˆï¼Œæœ€å¤§å¹¶å‘æ•°: {self.max_workers}")
        if self.search_service.is_available:
            logger.info("æœç´¢æœåŠ¡å·²å¯ç”¨ (Tavily/SerpAPI)")
        else:
            logger.warning("æœç´¢æœåŠ¡æœªå¯ç”¨ï¼ˆæœªé…ç½® API Keyï¼‰")

    # === æ–°å¢æ–¹æ³•ï¼šè¯»å–å¹¶ç­›é€‰ TrendRadar æ–°é—» ===
    def _get_trend_radar_context(self, code: str, json_path: str = 'news_summary.json') -> str:
        """è¯»å–ä¸Šæ¸¸ Action ç”Ÿæˆçš„æ–°é—»æ–‡ä»¶ï¼Œå¹¶æ ¹æ®è¡Œä¸šè¿›è¡Œç­›é€‰"""
        if not os.path.exists(json_path):
            return ""

        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                news_items = json.load(f)
            
            clean_code = code.split('.')[0] 
            target_sector = STOCK_SECTOR_MAP.get(clean_code, DEFAULT_SECTOR)
            
            filtered_news = []
            for item in news_items:
                category = item.get('category', 'Macro') 
                if category == 'Macro' or category == target_sector:
                    title = item.get('title', 'æ— æ ‡é¢˜')
                    summary = item.get('summary', '')
                    filtered_news.append(f"- ã€{category}ã€‘{title}: {summary}")

            if not filtered_news:
                return ""
            
            return "ã€æ¥è‡ª TrendRadar çš„è¡Œä¸šä¸å®è§‚ç®€æŠ¥ã€‘\n" + "\n".join(filtered_news) + "\n"
            
        except Exception as e:
            logger.warning(f"[{code}] è¯»å– TrendRadar æ–°é—»å¤±è´¥: {e}")
            return ""
    # ==========================================
    
    def fetch_and_save_stock_data(self, code: str, force_refresh: bool = False) -> Tuple[bool, Optional[str]]:
        """è·å–å¹¶ä¿å­˜å•åªè‚¡ç¥¨æ•°æ®"""
        try:
            today = date.today()
            if not force_refresh and self.db.has_today_data(code, today):
                logger.info(f"[{code}] ä»Šæ—¥æ•°æ®å·²å­˜åœ¨ï¼Œè·³è¿‡è·å–")
                return True, None
            
            logger.info(f"[{code}] å¼€å§‹ä»æ•°æ®æºè·å–æ•°æ®...")
            df, source_name = self.fetcher_manager.get_daily_data(code, days=30)
            
            if df is None or df.empty:
                return False, "è·å–æ•°æ®ä¸ºç©º"
            
            saved_count = self.db.save_daily_data(df, code, source_name)
            logger.info(f"[{code}] æ•°æ®ä¿å­˜æˆåŠŸï¼ˆæ¥æº: {source_name}ï¼Œæ–°å¢ {saved_count} æ¡ï¼‰")
            return True, None
        except Exception as e:
            error_msg = f"è·å–/ä¿å­˜æ•°æ®å¤±è´¥: {str(e)}"
            logger.error(f"[{code}] {error_msg}")
            return False, error_msg
    
    def analyze_stock(self, code: str) -> Optional[AnalysisResult]:
        """åˆ†æå•åªè‚¡ç¥¨"""
        try:
            stock_name = STOCK_NAME_MAP.get(code, '')
            
            # Step 1: è·å–å®æ—¶è¡Œæƒ…
            realtime_quote: Optional[RealtimeQuote] = None
            try:
                realtime_quote = self.akshare_fetcher.get_realtime_quote(code)
                if realtime_quote:
                    if realtime_quote.name: stock_name = realtime_quote.name
                    logger.info(f"[{code}] {stock_name} å®æ—¶è¡Œæƒ…: ä»·æ ¼={realtime_quote.price}")
            except Exception as e:
                logger.warning(f"[{code}] è·å–å®æ—¶è¡Œæƒ…å¤±è´¥: {e}")
            
            if not stock_name: stock_name = f'è‚¡ç¥¨{code}'
            
            # Step 2: è·å–ç­¹ç åˆ†å¸ƒ
            chip_data: Optional[ChipDistribution] = None
            try:
                chip_data = self.akshare_fetcher.get_chip_distribution(code)
                if chip_data: logger.info(f"[{code}] ç­¹ç åˆ†å¸ƒ: è·åˆ©={chip_data.profit_ratio:.1%}")
            except Exception as e:
                logger.warning(f"[{code}] è·å–ç­¹ç åˆ†å¸ƒå¤±è´¥: {e}")
            
            # Step 3: è¶‹åŠ¿åˆ†æ
            trend_result: Optional[TrendAnalysisResult] = None
            try:
                context = self.db.get_analysis_context(code)
                if context and 'raw_data' in context:
                    import pandas as pd
                    raw_data = context['raw_data']
                    if isinstance(raw_data, list) and len(raw_data) > 0:
                        df = pd.DataFrame(raw_data)
                        trend_result = self.trend_analyzer.analyze(df, code)
                        logger.info(f"[{code}] è¶‹åŠ¿åˆ†æ: {trend_result.trend_status.value}")
            except Exception as e:
                logger.warning(f"[{code}] è¶‹åŠ¿åˆ†æå¤±è´¥: {e}")
            
            # Step 4: å¤šç»´åº¦æƒ…æŠ¥æœç´¢
            news_context = None
            if self.search_service.is_available:
                logger.info(f"[{code}] å¼€å§‹å¤šç»´åº¦æƒ…æŠ¥æœç´¢...")
                intel_results = self.search_service.search_comprehensive_intel(
                    stock_code=code, stock_name=stock_name, max_searches=3
                )
                if intel_results:
                    news_context = self.search_service.format_intel_report(intel_results, stock_name)
                    logger.info(f"[{code}] æƒ…æŠ¥æœç´¢å®Œæˆ")
            else:
                logger.info(f"[{code}] æœç´¢æœåŠ¡ä¸å¯ç”¨ï¼Œè·³è¿‡")

            # === ã€æ’å…¥ç‚¹ã€‘æ³¨å…¥ TrendRadar æ–°é—»ä¸Šä¸‹æ–‡ ===
            trend_news = self._get_trend_radar_context(code)
            if trend_news:
                logger.info(f"[{code}] å·²æ³¨å…¥ TrendRadar è¡Œä¸šèˆ†æƒ…")
                if news_context is None:
                    news_context = ""
                news_context = trend_news + "\n" + news_context
            # ========================================
            
            # Step 5: è·å–åˆ†æä¸Šä¸‹æ–‡
            context = self.db.get_analysis_context(code)
            if context is None:
                logger.warning(f"[{code}] æ— æ³•è·å–åˆ†æä¸Šä¸‹æ–‡ï¼Œè·³è¿‡åˆ†æ")
                return None
            
            # Step 6: å¢å¼ºä¸Šä¸‹æ–‡æ•°æ®
            enhanced_context = self._enhance_context(
                context, realtime_quote, chip_data, trend_result, stock_name
            )
            
            # Step 7: è°ƒç”¨ AI åˆ†æ
            result = self.analyzer.analyze(enhanced_context, news_context=news_context)
            return result
            
        except Exception as e:
            logger.error(f"[{code}] åˆ†æå¤±è´¥: {e}")
            logger.exception(f"[{code}] è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
            return None
    
    def _enhance_context(self, context, realtime_quote, chip_data, trend_result, stock_name=""):
        enhanced = context.copy()
        if stock_name: enhanced['stock_name'] = stock_name
        elif realtime_quote and realtime_quote.name: enhanced['stock_name'] = realtime_quote.name
        
        if realtime_quote:
            enhanced['realtime'] = {
                'name': realtime_quote.name,
                'price': realtime_quote.price,
                'volume_ratio': realtime_quote.volume_ratio,
                'volume_ratio_desc': self._describe_volume_ratio(realtime_quote.volume_ratio),
                'turnover_rate': realtime_quote.turnover_rate,
                'pe_ratio': realtime_quote.pe_ratio,
                'pb_ratio': realtime_quote.pb_ratio,
                'total_mv': realtime_quote.total_mv,
                'circ_mv': realtime_quote.circ_mv,
                'change_60d': realtime_quote.change_60d,
            }
        
        if chip_data:
            current_price = realtime_quote.price if realtime_quote else 0
            enhanced['chip'] = {
                'profit_ratio': chip_data.profit_ratio,
                'avg_cost': chip_data.avg_cost,
                'concentration_90': chip_data.concentration_90,
                'concentration_70': chip_data.concentration_70,
                'chip_status': chip_data.get_chip_status(current_price),
            }
        
        if trend_result:
            enhanced['trend_analysis'] = {
                'trend_status': trend_result.trend_status.value,
                'ma_alignment': trend_result.ma_alignment,
                'trend_strength': trend_result.trend_strength,
                'bias_ma5': trend_result.bias_ma5,
                'bias_ma10': trend_result.bias_ma10,
                'volume_status': trend_result.volume_status.value,
                'volume_trend': trend_result.volume_trend,
                'buy_signal': trend_result.buy_signal.value,
                'signal_score': trend_result.signal_score,
                'signal_reasons': trend_result.signal_reasons,
                'risk_factors': trend_result.risk_factors,
            }
        return enhanced
    
    def _describe_volume_ratio(self, volume_ratio: float) -> str:
        if volume_ratio < 0.5: return "æåº¦èç¼©"
        elif volume_ratio < 0.8: return "æ˜æ˜¾èç¼©"
        elif volume_ratio < 1.2: return "æ­£å¸¸"
        elif volume_ratio < 2.0: return "æ¸©å’Œæ”¾é‡"
        elif volume_ratio < 3.0: return "æ˜æ˜¾æ”¾é‡"
        else: return "å·¨é‡"
    
    def process_single_stock(self, code: str, skip_analysis: bool = False, single_stock_notify: bool = False) -> Optional[AnalysisResult]:
        logger.info(f"========== å¼€å§‹å¤„ç† {code} ==========")
        try:
            success, error = self.fetch_and_save_stock_data(code)
            if not success: logger.warning(f"[{code}] æ•°æ®è·å–å¤±è´¥: {error}")
            
            if skip_analysis:
                logger.info(f"[{code}] è·³è¿‡ AI åˆ†æï¼ˆdry-run æ¨¡å¼ï¼‰")
                return None
            
            result = self.analyze_stock(code)
            if result:
                logger.info(f"[{code}] åˆ†æå®Œæˆ: {result.operation_advice}, è¯„åˆ† {result.sentiment_score}")
                if single_stock_notify and self.notifier.is_available():
                    try:
                        single_report = self.notifier.generate_single_stock_report(result)
                        if self.notifier.send(single_report): logger.info(f"[{code}] å•è‚¡æ¨é€æˆåŠŸ")
                        else: logger.warning(f"[{code}] å•è‚¡æ¨é€å¤±è´¥")
                    except Exception as e:
                        logger.error(f"[{code}] å•è‚¡æ¨é€å¼‚å¸¸: {e}")
            return result
        except Exception as e:
            logger.exception(f"[{code}] å¤„ç†è¿‡ç¨‹å‘ç”ŸæœªçŸ¥å¼‚å¸¸: {e}")
            return None
    
    def run(self, stock_codes: Optional[List[str]] = None, dry_run: bool = False, send_notification: bool = True) -> List[AnalysisResult]:
        start_time = time.time()
        if stock_codes is None:
            self.config.refresh_stock_list()
            stock_codes = self.config.stock_list
        
        if not stock_codes:
            logger.error("æœªé…ç½®è‡ªé€‰è‚¡åˆ—è¡¨")
            return []
        
        logger.info(f"===== å¼€å§‹åˆ†æ {len(stock_codes)} åªè‚¡ç¥¨ =====")
        logger.info(f"è‚¡ç¥¨åˆ—è¡¨: {', '.join(stock_codes)}")
        
        single_stock_notify = getattr(self.config, 'single_stock_notify', False)
        results: List[AnalysisResult] = []
        
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            future_to_code = {
                executor.submit(self.process_single_stock, code, dry_run, single_stock_notify and send_notification): code
                for code in stock_codes
            }
            for future in as_completed(future_to_code):
                code = future_to_code[future]
                try:
                    result = future.result()
                    if result: results.append(result)
                except Exception as e:
                    logger.error(f"[{code}] ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
        
        elapsed_time = time.time() - start_time
        logger.info(f"===== åˆ†æå®Œæˆ =====")
        logger.info(f"æˆåŠŸ: {len(results)}, è€—æ—¶: {elapsed_time:.2f} ç§’")
        
        if results and send_notification and not dry_run:
            if single_stock_notify:
                logger.info("å•è‚¡æ¨é€æ¨¡å¼ï¼šä»…ä¿å­˜æŠ¥å‘Šåˆ°æœ¬åœ°")
                self._send_notifications(results, skip_push=True)
            else:
                self._send_notifications(results)
        return results
    
    def _send_notifications(self, results: List[AnalysisResult], skip_push: bool = False) -> None:
        try:
            logger.info("ç”Ÿæˆå†³ç­–ä»ªè¡¨ç›˜æ—¥æŠ¥...")
            report = self.notifier.generate_dashboard_report(results)
            filepath = self.notifier.save_report_to_file(report)
            logger.info(f"å†³ç­–ä»ªè¡¨ç›˜æ—¥æŠ¥å·²ä¿å­˜: {filepath}")
            
            if skip_push: return
            
            if self.notifier.is_available():
                channels = self.notifier.get_available_channels()
                success = False
                if NotificationChannel.WECHAT in channels:
                    content = self.notifier.generate_wechat_dashboard(results)
                    success = self.notifier.send_to_wechat(content) or success
                
                for channel in channels:
                    if channel == NotificationChannel.WECHAT: continue
                    if channel == NotificationChannel.FEISHU: success = self.notifier.send_to_feishu(report) or success
                    elif channel == NotificationChannel.TELEGRAM: success = self.notifier.send_to_telegram(report) or success
                    elif channel == NotificationChannel.EMAIL: success = self.notifier.send_to_email(report) or success
                    elif channel == NotificationChannel.CUSTOM: success = self.notifier.send_to_custom(report) or success
                
                if success: logger.info("æ¨é€æˆåŠŸ")
                else: logger.warning("æ¨é€å¤±è´¥")
            else:
                logger.info("é€šçŸ¥æ¸ é“æœªé…ç½®")
        except Exception as e:
            logger.error(f"å‘é€é€šçŸ¥å¤±è´¥: {e}")


def parse_arguments() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description='Aè‚¡è‡ªé€‰è‚¡æ™ºèƒ½åˆ†æç³»ç»Ÿ')
    parser.add_argument('--debug', action='store_true', help='å¯ç”¨è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--dry-run', action='store_true', help='ä»…è·å–æ•°æ®')
    parser.add_argument('--stocks', type=str, help='æŒ‡å®šè‚¡ç¥¨ä»£ç ')
    parser.add_argument('--no-notify', action='store_true', help='ä¸å‘é€æ¨é€')
    parser.add_argument('--single-notify', action='store_true', help='å¯ç”¨å•è‚¡æ¨é€')
    parser.add_argument('--workers', type=int, default=None, help='å¹¶å‘çº¿ç¨‹æ•°')
    parser.add_argument('--schedule', action='store_true', help='å¯ç”¨å®šæ—¶ä»»åŠ¡')
    parser.add_argument('--market-review', action='store_true', help='ä»…è¿è¡Œå¤§ç›˜å¤ç›˜')
    parser.add_argument('--no-market-review', action='store_true', help='è·³è¿‡å¤§ç›˜å¤ç›˜')
    parser.add_argument('--webui', action='store_true', help='å¯åŠ¨WebUI')
    parser.add_argument('--webui-only', action='store_true', help='ä»…å¯åŠ¨WebUIæœåŠ¡')
    return parser.parse_args()


def run_market_review(notifier: NotificationService, analyzer=None, search_service=None) -> Optional[str]:
    logger.info("å¼€å§‹æ‰§è¡Œå¤§ç›˜å¤ç›˜åˆ†æ...")
    try:
        market_analyzer = MarketAnalyzer(search_service=search_service, analyzer=analyzer)
        review_report = market_analyzer.run_daily_review()
        if review_report:
            date_str = datetime.now().strftime('%Y%m%d')
            filepath = notifier.save_report_to_file(f"# ğŸ¯ å¤§ç›˜å¤ç›˜\n\n{review_report}", f"market_review_{date_str}.md")
            logger.info(f"å¤§ç›˜å¤ç›˜æŠ¥å‘Šå·²ä¿å­˜: {filepath}")
            if notifier.is_available():
                notifier.send(f"ğŸ¯ å¤§ç›˜å¤ç›˜\n\n{review_report}")
            return review_report
    except Exception as e:
        logger.error(f"å¤§ç›˜å¤ç›˜åˆ†æå¤±è´¥: {e}")
    return None


def run_full_analysis(config: Config, args: argparse.Namespace, stock_codes: Optional[List[str]] = None):
    try:
        if getattr(args, 'single_notify', False): config.single_stock_notify = True
        pipeline = StockAnalysisPipeline(config=config, max_workers=args.workers)
        
        results = pipeline.run(stock_codes=stock_codes, dry_run=args.dry_run, send_notification=not args.no_notify)
        
        market_report = ""
        if config.market_review_enabled and not args.no_market_review:
            review_result = run_market_review(pipeline.notifier, pipeline.analyzer, pipeline.search_service)
            if review_result: market_report = review_result
        
        try:
            feishu_doc = FeishuDocManager()
            if feishu_doc.is_configured() and (results or market_report):
                logger.info("æ­£åœ¨åˆ›å»ºé£ä¹¦äº‘æ–‡æ¡£...")
                tz_cn = timezone(timedelta(hours=8))
                now = datetime.now(tz_cn)
                doc_title = f"{now.strftime('%Y-%m-%d %H:%M')} å¤§ç›˜å¤ç›˜"
                full_content = ""
                if market_report: full_content += f"# ğŸ“ˆ å¤§ç›˜å¤ç›˜\n\n{market_report}\n\n---\n\n"
                if results:
                    dashboard_content = pipeline.notifier.generate_dashboard_report(results)
                    full_content += f"# ğŸš€ ä¸ªè‚¡å†³ç­–ä»ªè¡¨ç›˜\n\n{dashboard_content}"
                
                doc_url = feishu_doc.create_daily_doc(doc_title, full_content)
                if doc_url:
                    logger.info(f"é£ä¹¦äº‘æ–‡æ¡£åˆ›å»ºæˆåŠŸ: {doc_url}")
                    pipeline.notifier.send(f"[{now.strftime('%Y-%m-%d %H:%M')}] å¤ç›˜æ–‡æ¡£åˆ›å»ºæˆåŠŸ: {doc_url}")
        except Exception as e:
            logger.error(f"é£ä¹¦æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {e}")
            
    except Exception as e:
        logger.exception(f"åˆ†ææµç¨‹æ‰§è¡Œå¤±è´¥: {e}")


def main() -> int:
    args = parse_arguments()
    config = get_config()
    setup_logging(debug=args.debug, log_dir=config.log_dir)
    
    logger.info("=" * 60)
    logger.info("Aè‚¡è‡ªé€‰è‚¡æ™ºèƒ½åˆ†æç³»ç»Ÿ å¯åŠ¨")
    logger.info(f"è¿è¡Œæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info("=" * 60)
    
    config.validate()
    
    stock_codes = None
    if args.stocks:
        stock_codes = [code.strip() for code in args.stocks.split(',') if code.strip()]
    
    start_webui = (args.webui or args.webui_only or config.webui_enabled) and os.getenv("GITHUB_ACTIONS") != "true"
    if start_webui:
        try:
            from webui import run_server_in_thread
            run_server_in_thread(host=config.webui_host, port=config.webui_port)
        except Exception as e:
            logger.error(f"å¯åŠ¨ WebUI å¤±è´¥: {e}")
    
    if args.webui_only:
        try:
            while True: time.sleep(1)
        except KeyboardInterrupt:
            return 0

    try:
        if args.market_review:
            notifier = NotificationService()
            search_service = None
            analyzer = None
            if config.bocha_api_keys or config.tavily_api_keys:
                search_service = SearchService(bocha_keys=config.bocha_api_keys, tavily_keys=config.tavily_api_keys)
            if config.gemini_api_key:
                analyzer = GeminiAnalyzer(api_key=config.gemini_api_key)
            run_market_review(notifier, analyzer, search_service)
            return 0
        
        if args.schedule or config.schedule_enabled:
            from scheduler import run_with_schedule
            run_with_schedule(lambda: run_full_analysis(config, args, stock_codes), schedule_time=config.schedule_time, run_immediately=True)
            return 0
        
        run_full_analysis(config, args, stock_codes)
        
        if start_webui:
            try:
                while True: time.sleep(1)
            except KeyboardInterrupt: pass
        
        return 0
    except KeyboardInterrupt:
        return 130
    except Exception as e:
        logger.exception(f"ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())
